
@article{bischlResamplingMethodsMetaModel2012,
  title = {Resampling {{Methods}} for {{Meta}}-{{Model Validation}} with {{Recommendations}} for {{Evolutionary Computation}}},
  author = {Bischl, Bernd and Mersmann, O and Trautmann, Heike and Weihs, Claus},
  date = {2012-02-16},
  journaltitle = {Evolutionary Computation},
  shortjournal = {Evolutionary computation},
  volume = {20},
  pages = {249--75},
  doi = {10.1162/EVCO_a_00069},
  abstract = {Meta-modeling has become a crucial tool in solving expensive optimization problems. Much of the work in the past has focused on finding a good regression method to model the fitness function. Examples include classical linear regression, splines, neural networks, Kriging and support vector regression. This paper specifically draws attention to the fact that assessing model accuracy is a crucial aspect in the meta-modeling framework. Resampling strategies such as cross-validation, subsampling, bootstrapping, and nested resampling are prominent methods for model validation and are systematically discussed with respect to possible pitfalls, shortcomings, and specific features. A survey of meta-modeling techniques within evolutionary optimization is provided. In addition, practical examples illustrating some of the pitfalls associated with model selection and performance assessment are presented. Finally, recommendations are given for choosing a model validation technique for a particular setting.},
  file = {/home/marc/Zotero/storage/UPZVZ2KH/Bischl et al. - 2012 - Resampling Methods for Meta-Model Validation with .pdf}
}

@book{guyonFeatureExtractionFoundations2006,
  title = {Feature Extraction: Foundations and Applications},
  shorttitle = {Feature Extraction},
  author = {Guyon, Isabelle and Gunn, Steve and Nikravesh, Masoud and Zadeh, Lofti A.},
  date = {2006-07-20},
  edition = {2006 edition},
  publisher = {{Springer}},
  location = {{Berlin ; New York}},
  abstract = {This book is both a reference for engineers and scientists and a teaching resource, featuring tutorial chapters and research papers on feature extraction. Until now there has been insufficient consideration of feature selection algorithms, no unified presentation of leading methods, and no systematic comparisons.},
  file = {/home/marc/Zotero/storage/ULLNRCWD/Guyon et al. - 2006 - Feature Extraction Foundations and Applications.pdf},
  isbn = {978-3-540-35487-1},
  langid = {Englisch},
  pagetotal = {802}
}

@article{reunanenOverfittingMakingComparisons2003,
  title = {Overfitting in Making Comparisons between Variable Selection Methods},
  author = {Reunanen, Juha},
  date = {2003-03-01},
  journaltitle = {The Journal of Machine Learning Research},
  shortjournal = {J. Mach. Learn. Res.},
  volume = {3},
  pages = {1371--1382},
  issn = {1532-4435},
  abstract = {This paper addresses a common methodological flaw in the comparison of variable selection methods. A practical approach to guide the search or the selection process is to compute cross-validation performance estimates of the different variable subsets. Used with computationally intensive search algorithms, these estimates may overfit and yield biased predictions. Therefore, they cannot be used reliably to compare two selection methods, as is shown by the empirical results of this paper. Instead, like in other instances of the model selection problem, independent test sets should be used for determining the final performance. The claims made in the literature about the superiority of more exhaustive search algorithms over simpler ones are also revisited, and some of them infirmed.},
  file = {/home/marc/Zotero/storage/BM8JLA32/Reunanen - 2003 - Overfitting in making comparisons between variable.pdf},
  issue = {null}
}

@inproceedings{svetnikApplicationBreimanRandom2004,
  title = {Application of {{Breiman}}’s {{Random Forest}} to {{Modeling Structure}}-{{Activity Relationships}} of {{Pharmaceutical Molecules}}},
  booktitle = {Multiple {{Classifier Systems}}},
  author = {Svetnik, Vladimir and Liaw, Andy and Tong, Christopher and Wang, Ting},
  date = {2004-06-09},
  pages = {334--343},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-25966-4_33},
  url = {https://link.springer.com/chapter/10.1007/978-3-540-25966-4_33},
  urldate = {2020-03-28},
  abstract = {Leo Breiman’s Random Forest ensemble learning procedure is applied to the problem of Quantitative Structure-Activity Relationship (QSAR) modeling for pharmaceutical molecules. This entails using a...},
  eventtitle = {International {{Workshop}} on {{Multiple Classifier Systems}}},
  file = {/home/marc/Zotero/storage/PN3ITY9W/10.html},
  langid = {english}
}

@article{whitneyDirectMethodNonparametric1971,
  title = {A {{Direct Method}} of {{Nonparametric Measurement Selection}}},
  author = {Whitney, A.W.},
  date = {1971-09},
  journaltitle = {IEEE Transactions on Computers},
  volume = {C-20},
  pages = {1100--1103},
  issn = {2326-3814},
  doi = {10.1109/T-C.1971.223410},
  abstract = {A direct method of measurement selection is proposed to determine the best subset of d measurements out of a set of D total measurements. The measurement subset evaluation procedure directly employs a nonparametric estimate of the probability of error given a finite design sample set. A suboptimum measurement subset search procedure is employed to reduce the number of subsets to be evaluated. Teh primary advantage of the approach is the direct but nonparametric evaluation of measurement subsets, for the M class problem.},
  file = {/home/marc/Zotero/storage/PTRAP76P/Whitney - 1971 - A Direct Method of Nonparametric Measurement Selec.pdf},
  keywords = {Feature space evaluation; measurement (feature) selection; nearest neighbor rule; pattern classification.},
  note = {Conference Name: IEEE Transactions on Computers},
  number = {9}
}


